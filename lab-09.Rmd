---
title: "Lab 09: Algorithmic Bias"
author: "Lilly McClendon"
date: "04.10.2025"
output: github_document
---

## Load Necessary Packages  

First, let's load the necessary packages:  

```{r load-packages, message = FALSE}
library(tidyverse)
library(janitor)
library(fairness)
```

## Load the Data


```{r load_Compas_data}

compas <- read_csv("data/compas-scores-2-years.csv") %>%
  clean_names() %>%
  rename(decile_score = decile_score_12, 
         priors_count = priors_count_15)

glimpse(compas)

```

# Part 1: Exploring the data 

## Exercise 1 

```{r dimensions_compas_dataset}

nrow(compas)
ncol(compas)
```

The dimensions of the data set are 7214 x 53. There are 7214 rows of data. Each row in the data set represents a criminal defendant. The variables in this data set are id, first name, last name, the date they were compas screened, sex, birthday, age, age range, race, juvenile felony count, decile score, juvenile misdemeanor count, juvenile other count, priors count, how many days prior to screening the defendant was arrested, the date they went into jail for the compas screening, the date they left jail for the compas screening, their case number for the compas screening, arrest date for the compas screening, days from compas, the degree of their charge for the compas screening, a description of their charge for the compas screening, is recidivism, their case number for the recidivism case, the degree of their charge for the recidivism case, the days from arrest for the recidivism case, their offense date of the recidivism case, a description of their charge for the recidivism case, the date they entered jail for the recidivism case, the date they left jail for the recidivism case. There are also variables for violent recidivism, if violent recidivism occurred and coded, violent recidivism case number, violent recidivism charge degree, violent recidivism offense date, description of violent recidivism charge. There are variables for the type of assessment, the decile score, an interpretation of the decile score, date of screening, violence type assessment, violence decile score, violence score in text, violence screening date, date in custody, date left custody, prior counts, start, end, eent, and two-year recidivism. 

## Exercise 2 

```{r dimensions_compas_dataset_name}
library(dplyr)
distinct(compas, name)
```

There are 7,158 unique names in the compas data set. It may be that there are fewer unique defendants in the data set because there are defendants who have committed another crime and were screened again, or that some defendants could share a name. 

## Exercise 3 


```{r compas_distribution}
library(ggplot2)
ggplot(compas, aes(x=decile_score)) + 
  geom_histogram(binwidth = .5) + 
  scale_x_continuous(breaks = seq(0,13,1))
```

The distribution is a unimodal and positively skewed (right-skewed).The most common decile_score is 1. 

## Exercise 4 

```{r demographic_distribution_race}
library(ggplot2)
ggplot(compas, aes(x=race)) + 
  geom_bar(aes(x=forcats::fct_infreq(race))) + 
  theme(axis.text.x = element_text(angle = 20, vjust = .5))
```

```{r demographic_distribution_sex}
library(ggplot2)
ggplot(compas, aes(x=sex)) + 
  geom_bar(aes(x=forcats::fct_infreq(sex)))
```

```{r demographic_distribution_age_cat}
library(ggplot2)
ggplot(compas, aes(x=age_cat)) +
  geom_bar()
```

```{r demographic_distribution_age_cat_reorder}
library(ggplot2)
library(tidyverse)

agecat_compas <- subset(compas, select=c("age_cat"))

agecat_compas$age_cat <- factor(agecat_compas$age_cat, levels = c ("Less than 25", "25 - 45", "Greater than 45"))

ggplot(agecat_compas, aes(x=age_cat)) + 
  geom_bar() + 
  labs(x="Age Category", y = "Count")
```

# Part 2: Risk scores and recidivism

## Exercise 5 

```{r decile_score_and_recidivism_visualization}
library(ggplot2)

ggplot(compas, aes(x=decile_score, fill=factor(two_year_recid))) + 
  geom_histogram(binwidth = .5) +
  scale_x_continuous(breaks = seq(0,13,1)) + 
  scale_fill_discrete("Two Year Recidivism", labels = c("Did not recidivate", "Did recidivate"))
```

Based on this visualization, it seems that there is a slightly number of defendants who did recidivate in the higher decile scores than the lower decile scores. 

## Exercise 6

```{r accuracy_calculations}
library(dplyr)

accuracy_compas <- compas %>% 
  mutate(accuracy = case_when(
    decile_score >=7 & two_year_recid == 1 ~ 3, 
    decile_score <= 4 & two_year_recid == 0 ~ 3, 
    TRUE ~ 8
  ))

accuracy_compas %>% 
  count(accuracy, name = "count")

(4032 / 7214) * 100


accuracy_compas
```


## Exercise 7 

Overall, I calculated the accuracy of the COMPAS algorithm as 55.89%. This is not a formula that performs very well as it is barely over 50% accurate, meaning that it is better than chance, but not by much. 


# Part 3 Risk scores and recidivism

## Exercise 8 

```{r compas_distribution-by-race}
library(ggplot2)
library(dplyr)

race_compas <- compas %>%
  filter(race %in% c("African-American", "Caucasian"))
  
ggplot(race_compas, aes(x=decile_score, fill = race)) + 
  geom_histogram(binwidth = .5) + 
  scale_x_continuous(breaks = seq(0,13,1)) + 
  facet_wrap(~race)
```

The visualization for Caucasian defendants is right-skewed (positively skewed) meaning that there is a greater number of defendants with lower decile scores. The visualization for African-American defendants is unform indicating that the number of defendants with each decile score is approximately even. Additionally, there are quite a few more African-American defendants with high decile scores compared to the number of Caucasian defendants with high decile scores. 


## Exercise 9

```{r percentage_defendents_high_risk_calculation_by_race}
library(dplyr)

AA_defendants <- compas %>% 
  filter(race %in% c("African-American"))
count(AA_defendants)

sum(AA_defendants$decile_score >= 7)
sum(AA_defendants$decile_score <= 6)

(1425/3696)*100


C_defendants <- compas %>% 
  filter(race %in% c("Caucasian"))
count(C_defendants)
sum(C_defendants$decile_score >= 7)
sum(C_defendants$decile_score <= 6)

(419/2454)*100

```



There is a disparity in the percentage of African-American defendants and Caucasian defendants who were classified as high risk (decile_score >= 7). 38.56% of African-American defendants were classified as high risk while 17.07% of Caucasian defendants were classified as high risk. 

## Exercise 10 

```{r COMPAS-accuracy-different-racial-groups}

non_recidivists <- compas %>%
  filter(two_year_recid == 0)

recidivists <- compas %>%
  filter(two_year_recid == 1)

high_risk_non_recidivists <- non_recidivists %>% 
  filter(decile_score>=7)

low_risk_non_recidivists <- non_recidivists %>% 
  filter(decile_score<=4)

high_risk_recidivists <- recidivists %>% 
  filter(decile_score>=7)

low_risk_recidivists <- recidivists %>% 
  filter(decile_score<=4)

sum(non_recidivists$race == "African-American")
sum(high_risk_non_recidivists$race == "African-American")
(447/1795)*100

sum(non_recidivists$race == "Caucasian")
sum(high_risk_non_recidivists$race == "Caucasian")
(136/1488)*100

sum(recidivists$race == "African-American")
sum(low_risk_recidivists$race == "African-American")
(531/1901)*100

sum(recidivists$race == "Caucasian")
sum(low_risk_recidivists$race == "Caucasian")
(461/966)*100
```


False Positive Rate - African-Americans = 24.90% 

False Positive Rate - Caucasians = 9.14% 

False Negative Rate - African-Americans = 27.93%

False Negative Rate - Caucasians = 47.72 %

A false positive indicates that they thought the defendant would recidivate but they did not, while a false negative indicates that they thought the defendant would not recidivate, but the defendent did recidivate. 


## Exercise 11 

```{r falsepos-falseneg}

Misclassifications <- data.frame(
  stringsAsFactors = FALSE, 
  race = c("African-American", "Caucasian"),
  false_positive = c(24.90, 9.14), 
  false_negative = c(27.93, 47.72)
)

ggplot(Misclassifications, aes(x=race, y=false_positive, fill = race)) +
  geom_col() + 
  labs(title = "False Positive Rate by Race", y = "False Positive Rate (Percentage)")

ggplot(Misclassifications, aes(x=race, y=false_negative, fill = race)) +
  geom_col() + 
  labs(title = "False Negative Rate by Race", y = "False Negative Rate (Percentage)")

```

In the visualization with false positive rate there is much higher percentage among African-American defendants compared to Caucasian defendants and in the visualization with false negative rate, there is a much higher percentage among Caucasian defendants. This is problematic because a higher false negative rate indicates that among defendants who do not recidivate, African-American defendants are more likely to be categorized as high risk. A high false negative rate indicates that among defendants who do recidivate, Caucasian defendants are more likely to be categorized as low risk. The false positive and false negative rates show that African-American defendents are more likely to be erroneously classified as high risk and less likely to be erroneously classified as low risk.  


# Part 4: Understanding the sources of bias

## Exercise 12

```{r priors-risk}
library(ggplot2)

ggplot(compas, aes(fill=race, y=priors_count, x=decile_score)) + 
  geom_bar(position="stack", stat="identity") + 
  scale_x_continuous(breaks = seq(0,13,1)) 


ggplot(compas, aes(fill=race, y=priors_count, x=decile_score)) + 
  geom_bar(position="dodge", stat="identity")
```

The algorithm appears to weigh prior convictions more heavily for African-Americans than other races. 

## Exercise 13 

I see some evidence that supports ProPublica's claim that the algorithm is biased.When examining false positive rates and false negative rates, there is some obvious differences between African American defendants and Caucasian defendants. African American defendants have a higher rate of false positives than Caucasian defendants. This can be thought of as "false alarms" and in such it indicates that African-Americans at a higher rate than Caucasians are categorized as high risk when they do not go on to recidivate. Additionally, African-American defendants have a lower false negative rate than Caucasian defendants. This indicates that African-American defendants who go on to recidivate are less likely to be categorized as low risk. 

# Part 5: Designing fairer algorithms 

## Exercise 14 

I would work on trying to lower the biased type I and type II errors that disproportionately affect African-American defendants. I would try to filter for recent offenses rather than just any prior offense, and maybe take age into account. I would also try to reweigh some of the factors to see if that could help to reduce some of the bias. 


## Exercise 15 

When designing a "fair" algorithm, some trade offs may be that there is an inability to manage any biases that go into the data collected which will then be transferred to the results of the algorithm. Additionally, it may be hard to account for nuances about prior convictions etc. 

## Exercise 16 

There should not be reliance solely on alrogirthms. Judgement should be used when there are algorithms. Algorithms do not always account for other factors like mental health, etc.  

